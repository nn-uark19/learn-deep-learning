{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently at: https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "Crash course: https://developers.google.com/machine-learning/crash-course/\n",
    "        \n",
    "\n",
    "(source:  https://www.tensorflow.org/tutorials/keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "0. Intro to ML\n",
    "\n",
    "\n",
    "1. Framing\n",
    "    - Labels\n",
    "    - Features\n",
    "    - Examples\n",
    "    - Models\n",
    "    - Regression vs. classification\n",
    "    \n",
    "    \n",
    "2. Descending into ML\n",
    "    - Linear Regression\n",
    "    - Training and Loss\n",
    "        - Training\n",
    "        - Mean square error (MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Reducing Loss\n",
    "    - An Iterative Approach\n",
    "    - Gradient Descent: pick random value for w, then calculates the gradient of the loss curve at the starting point\n",
    "    - Learning Rate (also sometimes called step size)\n",
    "    - Optimizing Learning Rate\n",
    "    - Stochastic Gradient Descent\n",
    "        - batch: The set of examples used in one iteration (that is, one gradient update) of model training.\n",
    "        - batchsize: The number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. \n",
    "        - Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\n",
    "        - Mini-batch stochastic gradient descent (mini-batch SGD) is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random.\n",
    "    - Playground Exercise\n",
    "\n",
    "\n",
    "4. First Steps with TF: toolkit\n",
    "    - Figure 1. TensorFlow toolkit hierarchy.\n",
    "    - tf.estimator API\n",
    "    - graph: Nodes in the graph represent operations. Edges are directed and represent passing the result of an operation (a Tensor) as an operand to another operation. Use TensorBoard to visualize a graph.\n",
    "    - steps, which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model's weights once.\n",
    "    \n",
    "    - epoch: A full training pass over the entire data set such that each example has been seen once. Thus, an epoch represents N/batch size training iterations, where N is the total number of examples.\n",
    "    - iteration: A single update of a model's weights during training. An iteration consists of computing the gradients of the parameters with respect to the loss on a single batch of data.\n",
    "    - batch size, which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
