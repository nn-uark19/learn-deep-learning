{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently at: https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "Crash course: https://developers.google.com/machine-learning/crash-course/\n",
    "        \n",
    "\n",
    "(source:  https://www.tensorflow.org/tutorials/keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "0. Intro to ML\n",
    "\n",
    "\n",
    "## 1. Framing\n",
    "- Objectives\n",
    "    - Refresh the fundamental machine learning terms.\n",
    "    - Explore various uses of machine learning.\n",
    "- Labels\n",
    "- Features\n",
    "- Examples\n",
    "- Models\n",
    "- Regression vs. classification\n",
    "    \n",
    "    \n",
    "## 2. Descending into ML\n",
    "- Objectives\n",
    "    - Refresh your memory on line fitting.\n",
    "    - Relate weights and biases in machine learning to slope and offset in line fitting.\n",
    "    - Understand \"loss\" in general and squared loss in particular.\n",
    "- Linear Regression\n",
    "- Training and Loss\n",
    "    - Training\n",
    "    - Mean square error (MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reducing Loss\n",
    "- Objectives\n",
    "    - Discover how to train a model using an iterative approach.\n",
    "    - Understand full gradient descent and some variants, including: mini-batch gradient descent, stochastic gradient descent\n",
    "    - Experiment with learning rate.\n",
    "- An Iterative Approach\n",
    "- Gradient Descent: pick random value for w, then calculates the gradient of the loss curve at the starting point\n",
    "- Learning Rate (also sometimes called step size)\n",
    "- Optimizing Learning Rate\n",
    "- Stochastic Gradient Descent\n",
    "    - batch: The set of examples used in one iteration (that is, one gradient update) of model training.\n",
    "    - batchsize: The number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. \n",
    "    - Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\n",
    "    - Mini-batch stochastic gradient descent (mini-batch SGD) is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random.\n",
    "- Playground Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. First Steps with TF: toolkit\n",
    "- Objectives\n",
    "    - Learn how to create and modify tensors in TensorFlow.\n",
    "    - Learn the basics of pandas.\n",
    "    - Develop linear regression code with one of TensorFlow's high-level APIs.\n",
    "    - Experiment with learning rate.\n",
    "- Figure 1. TensorFlow toolkit hierarchy.\n",
    "- tf.estimator API\n",
    "- graph: Nodes in the graph represent operations. Edges are directed and represent passing the result of an operation (a Tensor) as an operand to another operation. Use TensorBoard to visualize a graph.\n",
    "- steps, which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model's weights once.\n",
    "- Others\n",
    "    - epoch: A full training pass over the entire data set such that each example has been seen once. Thus, an epoch represents N/batch size training iterations, where N is the total number of examples. = one forward pass and one backward pass of all the training examples\n",
    "    - iteration: A single update of a model's weights during training. An iteration consists of computing the gradients of the parameters with respect to the loss on a single batch of data.\n",
    "    - batch size, which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1 = # of images/step\n",
    "    - For instance if you have 20,000 images and a batch size of 100 then the epoch should contain 20,000 / 100 = 200 steps/iterations\n",
    "    - As an example, if you have 2,000 images and use a batch size of 10 an epoch consists of 2,000 images / (10 images / step) = 200 steps.\n",
    "- total # of training examples = batch_size * steps\n",
    "- Number of training examples in each period = batch_size * steps / periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generalization\n",
    "- Objective\n",
    "    - Develop intuition about overfitting.\n",
    "    - Determine whether a model is good or not.\n",
    "    - Divide a data set into a training set and a test set.\n",
    "    \n",
    "\n",
    "## 6. Training and Test Sets\n",
    "- Objectives\n",
    "    - Examine the benefits of dividing a data set into a training set and a test set.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation\n",
    "- Objectives\n",
    "    - Understand the importance of a validation set in a partitioning scheme.\n",
    "    - Use multiple features, instead of a single feature, to further improve the effectiveness of a model\n",
    "    - Debug issues in model input data\n",
    "    - Use a test data set to check if a model is overfitting the validation data\n",
    "    \n",
    "## 8. Representation\n",
    "- Objectives\n",
    "    - Map fields from logs and protocol buffers into useful ML features.\n",
    "    - Determine which qualities comprise great features.\n",
    "    - Handle outlier features.\n",
    "    - Investigate the statistical properties of a data set.\n",
    "    - Train and evaluate a model with tf.estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
