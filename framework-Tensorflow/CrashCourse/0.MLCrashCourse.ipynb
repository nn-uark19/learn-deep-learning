{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently at: https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "Crash course: https://developers.google.com/machine-learning/crash-course/\n",
    "        \n",
    "\n",
    "(source:  https://www.tensorflow.org/tutorials/keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "0. Intro to ML\n",
    "\n",
    "\n",
    "## 1. Framing\n",
    "- Objectives\n",
    "    - Refresh the fundamental machine learning terms.\n",
    "    - Explore various uses of machine learning.\n",
    "- Labels\n",
    "- Features\n",
    "- Examples\n",
    "- Models\n",
    "- Regression vs. classification\n",
    "    \n",
    "    \n",
    "## 2. Descending into ML\n",
    "- Objectives\n",
    "    - Refresh your memory on line fitting.\n",
    "    - Relate weights and biases in machine learning to slope and offset in line fitting.\n",
    "    - Understand \"loss\" in general and squared loss in particular.\n",
    "- Linear Regression\n",
    "- Training and Loss\n",
    "    - Training\n",
    "    - Mean square error (MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reducing Loss\n",
    "- Objectives\n",
    "    - Discover how to train a model using an iterative approach.\n",
    "    - Understand full gradient descent and some variants, including: mini-batch gradient descent, stochastic gradient descent\n",
    "    - Experiment with learning rate.\n",
    "- An Iterative Approach\n",
    "- Gradient Descent: pick random value for w, then calculates the gradient of the loss curve at the starting point\n",
    "- Learning Rate (also sometimes called step size)\n",
    "- Optimizing Learning Rate\n",
    "- Stochastic Gradient Descent\n",
    "    - batch: The set of examples used in one iteration (that is, one gradient update) of model training.\n",
    "    - batchsize: The number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. \n",
    "    - Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\n",
    "    - Mini-batch stochastic gradient descent (mini-batch SGD) is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random.\n",
    "- Playground Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. First Steps with TF: toolkit\n",
    "- Programming example\n",
    "    - Create a synthetic feature that is the ratio of two other features\n",
    "    - Use this new feature as an input to a linear regression model\n",
    "    - Improve the effectiveness of the model by identifying and clipping (removing) outliers out of the input data\n",
    "- Objectives\n",
    "    - Learn how to create and modify tensors in TensorFlow.\n",
    "    - Learn the basics of pandas.\n",
    "    - Develop linear regression code with one of TensorFlow's high-level APIs.\n",
    "    - Experiment with learning rate.\n",
    "- Figure 1. TensorFlow toolkit hierarchy.\n",
    "- tf.estimator API\n",
    "- graph: Nodes in the graph represent operations. Edges are directed and represent passing the result of an operation (a Tensor) as an operand to another operation. Use TensorBoard to visualize a graph.\n",
    "- steps, which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model's weights once.\n",
    "- Others\n",
    "    - epoch: A full training pass over the entire data set such that each example has been seen once. Thus, an epoch represents N/batch size training iterations, where N is the total number of examples. = one forward pass and one backward pass of all the training examples\n",
    "    - iteration: A single update of a model's weights during training. An iteration consists of computing the gradients of the parameters with respect to the loss on a single batch of data.\n",
    "    - batch size, which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1 = # of images/step\n",
    "    - For instance if you have 20,000 images and a batch size of 100 then the epoch should contain 20,000 / 100 = 200 steps/iterations\n",
    "    - As an example, if you have 2,000 images and use a batch size of 10 an epoch consists of 2,000 images / (10 images / step) = 200 steps.\n",
    "- total # of training examples = batch_size * steps\n",
    "- Number of training examples in each period = batch_size * steps / periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generalization\n",
    "- Objective\n",
    "    - Develop intuition about overfitting.\n",
    "    - Determine whether a model is good or not.\n",
    "    - Divide a data set into a training set and a test set.\n",
    "- DEF generalization: Refers to your model's ability to make correct predictions on new, previously unseen data as opposed to the data used to train the model.\n",
    "\n",
    "\n",
    "## 6. Training and Test Sets\n",
    "- Objectives\n",
    "    - Examine the benefits of dividing a data set into a training set and a test set.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation\n",
    "- Programming example\n",
    "    - Use multiple features, instead of a single feature, to further improve the effectiveness of a model\n",
    "    - Debug issues in model input data\n",
    "    - Use a test data set to check if a model is overfitting the validation data\n",
    "- Objectives\n",
    "    - Understand the importance of a validation set in a partitioning scheme.\n",
    "    \n",
    "    \n",
    "## 8. Representation\n",
    "- Programming example: Feature Sets\n",
    "    - Create a minimal set of features that performs just as well as a more complex feature set\n",
    "- Objectives\n",
    "    - Map fields from logs and protocol buffers into useful ML features.\n",
    "    - Determine which qualities comprise great features.\n",
    "    - Handle outlier features.\n",
    "    - Investigate the statistical properties of a data set.\n",
    "    - Train and evaluate a model with tf.estimator.\n",
    "- DEF representation- The process of mapping data to useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Crosses\n",
    "- Programming examples\n",
    "    - Improve a linear regression model with the addition of additional synthetic features (this is a continuation of the previous exercise)\n",
    "    - Use an input function to convert pandas DataFrame objects to Tensors and invoke the input function in fit() and predict() operations\n",
    "    - Use the FTRL optimization algorithm for model training\n",
    "    - Create new synthetic features through one-hot encoding, binning, and feature crosses\n",
    "- Objectives\n",
    "    - Build an understanding of feature crosses.\n",
    "    - Implement feature crosses in TensorFlow.\n",
    "- DEF feature cross- a synthetic feature that encodes nonlinearity in the feature space by multiplying two or more input features together.\n",
    "- Linear learners scale well to massive data. Using feature crosses on massive data sets is one efficient strategy for learning highly complex models. Neural networks provide another strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Regularization for Simplicity\n",
    "- Objectives\n",
    "    - Learn about trade-offs between complexity and generalizability.\n",
    "    - Experiment with L2 regularization.\n",
    "- L2 regulization: A type of regularization that penalizes weights in proportion to the sum of the squares of the weights. \n",
    "    - Encourages weight values toward 0 (but not exactly 0)\n",
    "    - Encourages the mean of the weights toward 0, with a normal (bell-shaped or Gaussian) distribution.\n",
    "- lambda: tune the overall impact of the regularization term by multiplying its value by a scalar known as lambda\n",
    "    - The ideal value of lambda produces a model that generalizes well to new, previously unseen data. \n",
    "    - Strong L2 regularization values tend to drive feature weights closer to 0. Lower learning rates (with early stopping) often produce the same effect because the steps away from 0 aren't as large\n",
    "\n",
    "\n",
    "## 11. Logistic Regression\n",
    "- Objectives\n",
    "    - Understand logistic regression.\n",
    "    - Explore loss and regularization functions for logistic regression.\n",
    "- **Linear vs Logistics Regression**: In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values.\n",
    "    - For instance, if X contains the area in square feet of houses, and Y contains the corresponding sale price of those houses, you could use linear regression to predict selling price as a function of house size. While the possible selling price may not actually be any, there are so many possible values that a linear regression model would be chosen.\n",
    "    - If, instead, you wanted to predict, based on size, whether a house would sell for more than 200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than 200K, or No, the house will not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Classification\n",
    "- **Skipped some parts**\n",
    "- Programming example\n",
    "    - Reframe the median house value predictor (from the preceding exercises) as a binary classification model\n",
    "    - Compare the effectiveness of logisitic regression vs linear regression for a binary classification problem\n",
    "- Objectives\n",
    "    - Evaluating the accuracy and precision of a logistic regression model.\n",
    "    - Understanding ROC Curves and AUCs.\n",
    "- Accuracy\n",
    "    - Precision: What proportion of positive identifications was actually correct?\n",
    "    - Recall: What proportion of actual positives was identified correctly?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Regularization: Sparsity\n",
    "- **Skipped some parts**\n",
    "- Programming example\n",
    "    - Calculate the size of a model\n",
    "    - Apply L1 regularization to reduce the size of a model by increasing sparsity\n",
    "- Objectives\n",
    "    - Learn how to drive uninformative coefficient values to exactly 0, in order to save RAM.\n",
    "    - Learn about other kinds of regularization besides L2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Intro to Neural Nets\n",
    "- Programmign example\n",
    "    - Define a neural network (NN) and its hidden layers using the TensorFlow DNNRegressor class\n",
    "    - Train a neural network to learn nonlinearities in a dataset and achieve better performance than a linear regression model\n",
    "- Objectives\n",
    "    - Develop some intuition about neural networks, particularly about: hidden layers, activation functions\n",
    "    \n",
    "## 15. Training Neural Networks\n",
    "- Objectives\n",
    "    - Develop some intuition around backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
